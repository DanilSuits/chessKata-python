# Test Calibration

At this point, we want a GREEN bar; we've already determined that the
test, as written, will flag an incorrect implementation.  We next need
to establish that it will bless a correct implementation.

## Constraints

First; if the tests are passing, then there is a promise that at least
some business value is available.

Second; the specific assurance that we want is that _correct_ implementations
satisfy the check.

Third: we want to minimize the amount of wall clock time that passes
during this stage of the exercise.

It follows that the simplest thing to do here is hard code a correct result.

## Discussion

Back in the day, this step _really_ messed people up.

I believe that part of the problem was, at the time, we really didn't have
a spelling of `Test Calibration`.  `Red-Green-Refactor` was the mantra,
which is (with the benefit of hindsight) both overly simplified, and
_really badly named_.

Also, this exercise came at a time when a common activity was to see
if you could design a sequence of tests that would "drive" toward a
particular solution.

We didn't have the notion of property based testing at that point,
although in the discussions you can see people groping around looking
for it.  Instead, all of our tests were example based, and the examples
chosen to get a first test going were _arbitrary_, which people correctly
identified as a code smell.

Today, we might say that coupling the implementation to specific
examples is [overfitting][1], but we didn't have that language yet either.

[1]:https://drive.google.com/file/d/0B59Tysg-nEQZOGhsU0U5QXo0Sjg/view?pref=2&pli=1
